{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16228bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This notebook, consists my version of reference bpe implementation  vs optimized python implementation vs huggingface tokeninzers implementation\n",
    "vs tiktoken implementation vs rust based bpe implementation by Karpathy.\n",
    "\"\"\"\n",
    "\n",
    "# imports \n",
    "import os \n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np \n",
    "import time\n",
    "import rustbpe \n",
    "import tiktoken \n",
    "import pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4607665",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT4_SPLIT_REGEX = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]|\\s[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "# \\s denotes space and | denotes OR operator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ab4e45",
   "metadata": {},
   "source": [
    "Simple space based tokenizer implementation for understanding the basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "36176a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20479 I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no g\n",
      "Total tokens after splitting using GPT4 regex: 4068\n",
      "['I', ' HAD', ' always', ' thought', ' Jack', ' Gisburn', ' rather', ' a', ' cheap', ' genius', '-though', ' a', ' good', ' fellow', ' enough', '-so', ' it', ' was', ' no', ' great']\n"
     ]
    }
   ],
   "source": [
    "import regex as re  # IMPORTANT: use 'regex', not 're'\n",
    "\n",
    "with open(\"the-verdict.txt\", 'r', encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(len(raw_text), raw_text[:100])\n",
    "\n",
    "result = re.findall(GPT4_SPLIT_REGEX, raw_text)\n",
    "\n",
    "# we can also define an regex for splitting based on spaces and puntuations \n",
    "# TEST_REGEX = r\"[,.;:]|--|\\s\"\n",
    "# result_test = re.split(TEST_REGEX, raw_text)\n",
    "# result_test = [token for token in result_test if token]\n",
    "# print(len(result_test), result_test[:20])\n",
    "\n",
    "# Remove empty strings\n",
    "result = [token for token in result if token]\n",
    "\n",
    "print(f\"Total tokens after splitting using GPT4 regex: {len(result)}\")\n",
    "print(result[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e256685a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 1252\n",
      "('\\n', 0)\n",
      "(' ', 1)\n",
      "(' .\\n', 2)\n",
      "(' .\"\\n', 3)\n",
      "(' A', 4)\n",
      "(' Among', 5)\n",
      "(' And', 6)\n",
      "(' Arrt', 7)\n",
      "(' At', 8)\n",
      "(' Burlington', 9)\n",
      "(' But', 10)\n",
      "(' By', 11)\n",
      "(' Carlo', 12)\n",
      "(' Chicago', 13)\n",
      "(' Claude', 14)\n",
      "(' Croft', 15)\n",
      "(' Devonshire', 16)\n",
      "(' Don', 17)\n",
      "(' Dubarry', 18)\n",
      "(' Emperors', 19)\n",
      "(' Florence', 20)\n",
      "(' For', 21)\n",
      "(' Gallery', 22)\n",
      "(' Gideon', 23)\n",
      "(' Gisburn', 24)\n",
      "(' Grafton', 25)\n",
      "(' Greek', 26)\n",
      "(' Grindle', 27)\n",
      "(' HAD', 28)\n",
      "(' Had', 29)\n",
      "(' He', 30)\n",
      "(' Her', 31)\n",
      "(' Hermia', 32)\n",
      "(' His', 33)\n",
      "(' I', 34)\n",
      "(' If', 35)\n",
      "(' It', 36)\n",
      "(' Jack', 37)\n",
      "(' Jove', 38)\n",
      "(' Just', 39)\n",
      "(' Lord', 40)\n",
      "(' Made', 41)\n",
      "(' Miss', 42)\n",
      "(' Monte', 43)\n",
      "(' Mr', 44)\n",
      "(' Mrs', 45)\n",
      "(' My', 46)\n",
      "(' No', 47)\n",
      "(' Now', 48)\n",
      "(' Nutley', 49)\n",
      "(' Of', 50)\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(result))\n",
    "all_words.extend([\"<|unk|>\", \"<|endoftext|>\"])  # adding unknown token + endoftext token\n",
    "vocab_size = len(all_words)\n",
    "print(f\"Vocab size: {vocab_size}\") # before adding additonal tokens - 1250 \n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break\n",
    "\n",
    "\n",
    "# let's define a simple tokenizer based on the above vocab\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_id = vocab\n",
    "        self.id_to_str = { i:s for s, i in vocab.items() }\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.findall(GPT4_SPLIT_REGEX, text)\n",
    "        preprocessed = [token for token in preprocessed if token]\n",
    "        # replaces unknown words by <unk> token\n",
    "        preprocessed = [token if token in self.str_to_id else '<|unk|>' for token in preprocessed]\n",
    "\n",
    "        return [self.str_to_id[token] for token in preprocessed]\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        txt = \" \".join([self.id_to_str[token_id] for token_id in token_ids])\n",
    "        txt = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', txt)\n",
    "        return txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "29c5fd5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [1250, 34, 1250, 1250, 1, 1032, 1064, 613, 1250, 549, 1064, 1250, 125, 648, 1250, 1250, 538, 930, 466, 1250, 1250, 1250, 1, 1020, 1020, 1, 459, 855, 1250, 538, 459]\n",
      "Decoded: <|unk|>  I <|unk|> <|unk|>    which  you  may <|unk|>  know  you <|unk|>  and  my <|unk|> <|unk|>  is  the  greatest <|unk|> <|unk|> <|unk|>    water  water    good  so <|unk|>  is  good\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizer(vocab)\n",
    "test_text = \"Hi, I am idiot - which you may already know, you dumbassmf, and my favourite webtoon is the greatest_estate developer!!! lloyd = water, water = good, so lloyd is good.\"\n",
    "\n",
    "encoded = tokenizer.encode(test_text)\n",
    "print(f\"Encoded: {encoded}\")\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(f\"Decoded: {decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96662ea2",
   "metadata": {},
   "source": [
    "Reference BPE Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eae06a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(1, 2): 2, (2, 3): 3, (3, 2): 1, (3, 4): 1, (4, 1): 1}\n"
     ]
    }
   ],
   "source": [
    "# get stats function\n",
    "\n",
    "def get_stats(token_ids, counts=None):\n",
    "    \"\"\"\n",
    "    Given a list of token_ids, return a dictionary of pair frequencies, \n",
    "    which are then used to merge based on highest frequency.\n",
    "    \"\"\"\n",
    "    counts = {} if counts is None else counts \n",
    "    for pair in zip(token_ids, token_ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts \n",
    "\n",
    "# def get_stats(token_ids, counts=None):\n",
    "#     \"\"\"\n",
    "#     Given a list of token_ids, return a dictionary of pair frequencies, \n",
    "#     which are then used to merge based on highest frequency.\n",
    "#     \"\"\"\n",
    "#     if counts is None:\n",
    "#         counts = defaultdict(int)\n",
    "#     else:\n",
    "#         counts = defaultdict(int, counts)\n",
    "    \n",
    "#     for pair in zip(token_ids, token_ids[1:]):\n",
    "#         counts[pair] += 1\n",
    "#     return counts\n",
    "\n",
    "# def get_stats(token_ids):\n",
    "#     return Counter(zip(token_ids, token_ids[1:]))\n",
    "\n",
    "# test\n",
    "test_token_ids = [1, 2, 3, 2, 3, 4, 1, 2, 3]\n",
    "pair_counts = get_stats(test_token_ids)\n",
    "print(pair_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5132619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 3, 2, 3, 4, 5, 3]\n"
     ]
    }
   ],
   "source": [
    "# merge_token_ids function \n",
    "\n",
    "def merge_token_ids(ids, pair, idx):\n",
    "    \"\"\"\n",
    "    In the list of token_ids, merge the given pair and assign it idx\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    new_ids = []\n",
    "    while i < len(ids):\n",
    "        # if not at the very end of the list and found the pair\n",
    "        if ids[i] == pair[0] and i < len(ids) - 1 and ids[i + 1] == pair[1]:\n",
    "            new_ids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_ids.append(ids[i])\n",
    "            i += 1\n",
    "    return new_ids\n",
    "\n",
    "# test \n",
    "pair = (1,2)\n",
    "idx = 5\n",
    "print(merge_token_ids(test_token_ids, pair, idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "04ee842c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Regex tokenizer implementation \n",
    "\n",
    "class RegexTokenizer:\n",
    "    \n",
    "    def __init__(self, pattern):\n",
    "        # pattern - optional string to override the default GPT4 regex pattern\n",
    "        # special_tokens - str -> int dict of special tokens used during tokenization\n",
    "        self.pattern = GPT4_SPLIT_REGEX if pattern is None else pattern \n",
    "        self.merges = {}\n",
    "        self.compiled_pattern = re.compile(self.pattern) # what does compile do? \n",
    "        self.special_tokens = {} # like \"<|unk|>\" or \"<|endoftext|>\"\n",
    "        self.inverse_special_tokens = {}\n",
    "        self.vocab = self._build_vocab()\n",
    "    \n",
    "    def _build_vocab(self):\n",
    "        # vocab is deterministic and is derived from the merges \n",
    "        vocab = {idx: bytes([idx]) for idx in range(256)} # initialize the vocab\n",
    "        for (p0, p1), idx in self.merges.items():\n",
    "            vocab[idx] = vocab[p0] + vocab[p1]\n",
    "        for special, idx in self.special_tokens.items():\n",
    "            vocab[idx] = special.encode(\"utf-8\")\n",
    "        return vocab\n",
    "\n",
    "    def train(self, text, vocab_size, verbose=True):\n",
    "        assert vocab_size >= 256 \n",
    "        num_merges = vocab_size - 256 \n",
    "\n",
    "        # keep track of whether at any point, during the training, the merge is ambigious - counts of pairs are not unique \n",
    "        # good technique \n",
    "\n",
    "        text_chunks = re.findall(self.compiled_pattern, text)\n",
    "\n",
    "        ids = [list(chunk.encode(\"utf-8\")) for chunk in text_chunks]\n",
    "\n",
    "        # iteratively merge the most common pairs to create new tokens until we reach the desired vocab_size or num_merges which is vocab_size - 256\n",
    "        merges = {}\n",
    "        vocab = {idx: bytes([idx]) for idx in range(256)} # idx -> bytes \n",
    "\n",
    "        for i in range(num_merges):\n",
    "            # count the no of times every consecutive pair appears\n",
    "            stats = {}\n",
    "            # chunk_ids = token_ids \n",
    "            for chunk_ids in ids:\n",
    "                # passing in stats will update in place, adding up counts - we are doing chunk_wise?\n",
    "                get_stats(chunk_ids, stats)\n",
    "            \n",
    "            # find the pair with the highest count \n",
    "            pair = max(stats, key=stats.get) #type: ignore \n",
    "            # check if the merge is ambiguous - i.e max value is not unique \n",
    "            pair_count = stats[pair]\n",
    "            pairs_with_max_count = [pair for pair, count in stats.items() if count == pair_count]\n",
    "            if len(pairs_with_max_count) > 1:\n",
    "                # we somehow have to break the tie \n",
    "                # print the top 10 pairs with their counts\n",
    "                # print(f\"{i} Merge is ambiguous! {pair} has {pair_count} occurrences\")\n",
    "                # for print_pair, print_count in sorted(stats.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "                #     print(f\"{print_pair}: {print_count}\")\n",
    "                ambiguous = True \n",
    "            # mint a new token: assign it the next available id \n",
    "            idx = 256 + i \n",
    "            # replace all occurences of pair with idx - done by the merge function \n",
    "            ids = [merge_token_ids(chunk_ids, pair, idx) for chunk_ids in ids]\n",
    "            # save the merge \n",
    "            merges[pair] = idx \n",
    "            vocab[idx] = vocab[pair[0]] + vocab[pair[1]]\n",
    "            # prints \n",
    "            if verbose:\n",
    "                print(f\"merge {i+1}/{num_merges}: {pair} -> {idx} ({vocab[idx]}) had {stats[pair]} occurrences\")\n",
    "        \n",
    "        # save class variables \n",
    "        self.merges = merges # used in encode() - updating from {}\n",
    "        self.vocab = vocab # used in decode() - updating from the initial 256 dict \n",
    "\n",
    "        return ambiguous \n",
    "    \n",
    "    def _encode_chunk(self, text_bytes):\n",
    "        # return the token_ids \n",
    "        # convert all the bytes into integers in range 0...255 \n",
    "        ids = list(text_bytes)\n",
    "        while len(ids) >=2:\n",
    "            # find the pair with the lowest marge index \n",
    "            stats = get_stats(ids)\n",
    "            pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
    "            # subtle: if there are no more merges available, the key will \n",
    "            # result in an inf for every single pair, and the min will be\n",
    "            # just the first pair in the list, arbitrarily we can detect this terminating case by a membership check.\n",
    "            if pair not in self.merges:\n",
    "                break # nothing else can be merged anymore \n",
    "            # otherwise let's merge the best pair\n",
    "            idx = self.merges[pair]\n",
    "            ids = merge_token_ids(ids, pair, idx)\n",
    "        return ids\n",
    "\n",
    "    def encode_ordinary(self, text):\n",
    "        # Encoding that ingnores any special tokens \n",
    "        # split text into chunks of text by catergories defined in regex pattern \n",
    "        text_chunks = re.findall(self.compiled_pattern, text)\n",
    "        ids = []\n",
    "        for chunk in text_chunks:\n",
    "            chunk_bytes = chunk.encode(\"utf-8\")\n",
    "            chunk_ids = self._encode_chunk(chunk_bytes)\n",
    "            ids.extend(chunk_ids)\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc8c4af",
   "metadata": {},
   "source": [
    "Faster Python tokenizer - optimized version of the reference tokenizer implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8b0c86df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just an clever implementation!\n",
    "def fast_merge_inplace_token_ids(ids, pair, idx):\n",
    "    \"\"\"\n",
    "    In the list of integers (ids), replace all the concurrent occurences of pair, with the new \n",
    "    integer token idx in place.\n",
    "    \"\"\"\n",
    "    # find all the positions where the pair occurs \n",
    "    i = 0\n",
    "    while i < len(ids) - 1:\n",
    "        if ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            ids[i] = idx \n",
    "            ids.pop(i+1)\n",
    "        else:\n",
    "            i += 1\n",
    "    return ids "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
